\chapter{Evaluation}
\label{chp:5}
\section{Introduction}
Inorder to address the extent to which the proposed domain independent summarisation system can perform extractive personalised summarisation, two forms of evaluation were performed, referred to as Analysis 1 and 2. 

Analysis 1 set out to determine the efficacy of the systems summarisation method, the system was tested on two summarisation data sets, a single document and multi-document dataset. From the summaries produced ROUGE metrics (Lin, Cao, Gao, and Nie, 2006) were calculated and compared to other extractive summarisation systems. The aim of the comparative evaluation was to determine both the quality of summaries produced against human generated summaries as well as the systems performance against state of the art extractive summarisation systems. 

Analysis 2 examines the systems ability to provide personalised summarisation, through the use of queries. Two types of queries were examined, a context free query and contextual query. Context free queries examine the system's ability to retrieve relevant documents and produce a human interpretable summary within a specific domain. The second type of query was to assess the efficacy of the system to be used in a recommender system from examining system summaries produced from a context and context free query in specific domains. The results of comparative evaluation show that the system does achieve state of the art summarisation performance, but is a competitive method for summarisation. The results of query based summarisation evaluation, demonstrate the system's effectiveness to be providing personalised summaries in a specific domain, and the possible use of the summarisation system in a recommender system.

\section{Analysis 1: Comparative Summarisation Evaluation}
The comparative evaluation of this system with state of the art are extractive summarisation systems assess the implementation of existing methods in the system to perform generic summarisation. The metrics to use in the comparative evaluation are the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metrics \citep{lin2004rouge}. ROUGE metrics are used for evaluating how well automatic summarisation methods can produce summaries similar to a human generated reference summary. The ROUGE metrics used are:

\begin{itemize}
    \item ROUGE-1: The overlap of uni-grams (single terms) between a system summary and a reference summary.
    \item ROUGE-2: The overlap of bi-grams (adjacent terms) between a system summary and a reference summary. This is provide by the following formula:
\end{itemize}

Each ROUGE metric uses two methods of scoring, precision and recall. Recall accounts for how well the system summary is able to cover the content of the reference summary. The precision of a summary asses how much of the system summary is relevant to the reverence summary. These two calculations can be generalised as the following equations:

\begin{equation*}
    ROUGE_{recall} = \frac{\#\:of\: Overlaps\: of \:System\: Content \:and \:Reference\: Content}{\#\: of\: Content\: in\: Reference\: Summary}
    \label{rouge_r2}
\end{equation*}

\begin{equation*}
    ROUGE_{precision} = \frac{\# \:of\: Overlaps\: of\: System\: Content\: and\: Reference\: Content}{\#\: of\: Content\: in\: System\: Summary}
    \label{rouge_p3}
\end{equation*}

A perfect recall score of 1.0 means that all content in the reference summary is included in the system summary, a high recall score (close to 1.0) alone cannot assess a summary. Summaries must be concise and producing the original document would give you a perfect recall score. A high precision score (close to 1.0) means that all of the content in the system summary is revelvent in the summary, but fails to account for missing content. A summary that contains only one sentence which is in the reference summary would have a perfect precision score. These two scores for a ROUGE metric are thus combined into a F-1 score which is the harmonic mean of precision and recall scores.

\begin{equation*}
    ROUGE_{F1} = \frac{2\times precision \times recall}{(precision + recall)}
    \label{rouge_f1}
\end{equation*}

The three ROUGE metrics, and their three scores, were used on the evaluation of the system on two summarisation datasets, a single document and a multi document set. The method for calculating ROUGE metrics as well as the results and observations are provided in the next two subsections.

\subsection{Method}
Inorder to calculate the ROUGE metrics the system must perform summarisation on documents that already have human generated summaries for reference. Summarisation datasets contain documents paired with reference summaries. Summarisation datasets that contain news articles were chosen to perform evaluation. News content is similar to the content of historical Wikipedia articles, as discussed in Section \ref{subsec:3.5.2}. Many state of the art systems also use news summarisation datasets, therefore evaluating the proposed system on the same dataset allows for a comparison of the proposed system with state of the art extractive systems.

The system was classified in Section \ref{sec:3.2} to summarise multiple documents. Despite this the system was evaluated on a single document and multi-document datasets. Single document summarisation datasets evaluation will determine if the system could also be used to perform single document summarisation. Examining the systems summarisation of single documents will highlight the implementation techniques made to handle redundancy that is more prevalent in multiple domain specific documents. Thus it is expected that the summarization system will perform worse on a single document summarisation from the implementation being multi-document and domain specific.

\subsubsection{Single Document Dataset}
To assess the systems performance of single document summarisation, the CNN/DailyMail non anonymised dataset \citep{see2017get} was used. This dataset contains CNN and DailyMail news articles with an abstract written by the author of the article. These abstracts are treated as summaries that cover the most salient content of the news article. This dataset contains 287,226 training pairs, 13,368 validation pairs and 11,490 test pairs. Many of the systems that use this dataset are based on neural networks and use both the training and validation article abstract pairs to form a model. This system did not use any of the training or validation pairs to adjust model parameters. The system was evaluated using a random selection of article abstract test pairs.

\subsubsection{Multi Document Dataset}
The assessment of the systems performance in performing multi-document summarisation was done using the MultiNews dataset \citep{fabbri2019multi}. This dataset contains multiple news articles paired with a human summary that covers the most salient content across all of the paired news articles. This dataset contains 44,972 training pairs, 5,622 validation pairs, and 5,622 test pairs. Similarly to the CNN/DailyMail dataset, the MultiNews dataset can be used for the training of neural networks, with training and validation being done on the corresponding paris. The system did not use any of the training or validation pairs for model formation. The system was evaluated using a random subset of test pairs. 

\subsubsection{Calculating ROUGE metrics}
For both the single document and multi-document summarisation datasets, 100 randomly selected test pairs were used. For each of the selected test pairs, the article(s) were used in creating a system summary. The three ROUGE metrics were then calculated from the system generated summary and the reference summary. The average of the three ROGUEF-1 scores was then calculated. The method is presented below:

\begin{enumerate}
    \item Extract the article(s) from test pair;
    \item Create a Corpus from article(s), using the Corpus class defined in Corpus.py;
    \item Create a Summary object using the Corpus.concepts data structure and the Corpus object, from the Summary class in Summary.py;
    \item From the Summary object create a summary of the same length as the reference;
    \item Extract the reference abstract from the test pair;
    \item Calculate ROUGE-1, ROUGE-2, and ROUGE-L, and their respective recall, precision, and F-1, scores using the reference and the system summary;
    \item Store the calculated scores in a python dictionary, using the test pair id as a key;
\end{enumerate}

ROUGE-1, ROUGE-2, and ROUGE-L score were calculated using the Rouge python package (Lin, 2004). This package calculates the ROUGE metrics of a provided reference summary as a string and a system summary as a string. 

Where f is the F-1 value, p is the precision, and r is the recall. The average F-1 score for ROUGE-1, ROUGE-2, and ROUGE-l was then calculated. The averages found from the 100 selected test pairs for both datasets are presented in the Table \ref{tab:singleResults} and Table \ref{tab:multiResults}. as percentages.

\begin{table}[h]
    \caption{Single Document: CNN/DailyMail Dataset Results}
    \label{tab:singleResults}
    \centering
    \begin{tabular}{l l l l}
    \toprule
    \textbf{Metric} & \textbf{Precision} & \textbf{Recall} & \textbf{F-1}\\
    \midrule
    ROUGE-1 & 24.24 & 41.74 & \textbf{28.72}\\
    ROUGE-2 & 7.50 & 13.68 & \textbf{9.22}\\
    ROUGE-l & 23.90 & 36.11 & \textbf{26.94}\\
    \bottomrule\\
    \end{tabular}
\end{table}

\begin{table}[h]
    \caption{Multi-Document: MultiNews Dataset Results}
    \label{tab:multiResults}
    \centering
    \begin{tabular}{l l l l}
    \toprule
    \textbf{Metric} & \textbf{Precision} & \textbf{Recall} & \textbf{F-1}\\
    \midrule
    ROUGE-1 & 50.90 & 26.02 & \textbf{32.78}\\
    ROUGE-2 & 26.26 & 8.87 & \textbf{10.92}\\
    ROUGE-l & 35.42 & 21.04 & \textbf{25.54}\\
    \bottomrule\\
    \end{tabular}
\end{table}

\subsection{Results \& Observation}
What is immediately clear from results from the two data sets is the lower scores of ROUGE-2 metrics compared to ROUGE-L and ROUGE-1. This is a result of the system only extracting noun pairs and named entities in concept extraction in the creation of a corpus, discussed in Section \ref{subsec:4.1.2}. This method of text extraction leaves out dependencies of words that would be captured with other methods of text extraction, such as verb noun pairs and adjectives and noun pairs. It should be noted that the reference summaries for both datasets are human created and are not created from using extraction of document sentences. Thus the scores here are limited by the method of summarisation being extractive. 

Another interesting comparison is the inverse performance of precision and recall in single document and multi-document summarisation. Single document summarisation has a higher recall and lower precision compared to multi-document summarisation. Multi-document summarisation has a lower recall and higher precision compared to single document summarisation. This inverse relationship can be accounted for via the compression ratio in summarisation, discussed in the issues of multi-document summarisation found in Section \ref{subsec:2.1.2}. The compression ratio is the summary length over source content length. A small compression ratio is when a short summary has to be produced via a large set of documents, as experienced in multi document summarisation. The lower the compression ratio the more difficult it is to provide all salient source content in a summary. The result of this challenge is shown directly in the inverse precision and recall relationship of single and multi-document summarisation results. For a single document summary recall is higher because it is easier to cover content most salient to the source document due to the more moderate compression ratio, precision is lower due to the amount of extract content included in a summary because of this more moderate compression ratio. Conversely multi-document summarisation deals with a much smaller compression ratio, making it easier to have a high precision, as most content in the system summary overlaps with the reference summary. At the same time a smaller compression ratio makes it much harder to have a high recall score because the challenge of presenting all salient content is made much more difficult by the increase in content to be summarised. Overall the system had better performance for multi-document summarisation than single document, this is reflective of the efforts made in selecting methods for the system to handle the compression ratio and redundancy problems of multi-document summarisation.

To truly assess the performance of the system it must be compared to ROUGE metrics of state of the art systems performing summarisation on the same datasets. The results for this comparison are given in Table \ref{tab:singleCompare} and Table \ref{tab:multiCompare}. ROUGE metrics for state of the art system on CNN/DailyMail were gathered from finding recent state of the art extractive summarisation methods that use the CNN/DailyMail dataset. The paper that introduced the MultiNews dataset includes a state of the art ROUGE metrics comparison of extractive systems. Their results are used for the comparison ROUGE of metrics of this system on the MultiNews Dataset.


\begin{table}[h]
    \caption{Single Document Comparative Performance on CNN/DailyMail Dataset}
    \label{tab:singleCompare}
    \centering
    \begin{tabular}{| L | l | l | l | L |}\hline
    \textbf{Method} & \textbf{Rouge-1} & \textbf{Rouge-2} & \textbf{Rouge-l} & \textbf{Paper} \Tstrut\Bstrut\\\hline
    BertSumExt \citep{liu2019text} & 43.85 & 20.34 & 39.90 & Text Summarization with Pretrained Encoders \Tstrut\Bstrut\\\hline
    PNBERT \citep{zhong2019searching} & 42.69 & 19.60 & 38.85 & Searching for Effective Neural Extractive Summarization: What Works and What’s Next \Tstrut\Bstrut\\\hline
    HIBERT \citep{zhang2019hibert} & 42.37 & 19.95 & 38.83 & HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization \Tstrut\Bstrut\\\hline
    NeuSUM \citep{zhou2018neural} & 41.59 & 19.01 & 37.98 & Neural Document Summarization by Jointly Learning to Score and Select Sentences \Tstrut\Bstrut\\\hline
    NMF-TR \citep{khurana2019extractive} & 34.20 & 13.12 & 31.00  & Extractive Document Summarization using Non-negative Matrix Factorization \Tstrut\Bstrut\\\hline
    Proposed System	& \textbf{28.17} & \textbf{9.90} & \textbf{26.94} & \Tstrut\Bstrut\\\hline
    \end{tabular}
\end{table}

\begin{table}[h]
    \caption{Multi-Document Comparative Performance on MultiNews Dataset}
    \label{tab:multiCompare}
    \centering
    \begin{tabular}{| L | l | l | L |} \hline
    \textbf{Method} & \textbf{Rouge-1} & \textbf{Rouge-2} & \textbf{Paper} \Tstrut\Bstrut\\\hline
    LexRank \citep{erkan2004lexrank} & 43.85 & 20.34  & Lexrank: Graph-based lexical centrality as salience in text summarization \Tstrut\Bstrut\\\hline
    TextRank \citep{mihalcea2004textrank} & 42.69 & 38.85 & Textrank: Bringing order into text \Tstrut\Bstrut\\\hline
    MMR \citep{carbonell1998use} & 42.37 & 19.95  & The use of MMR, diversity-based reranking for reordering documents and producing summaries \Tstrut\Bstrut\\\hline
    PG-MMR \citep{lebanoff2018adapting} & 41.59 & 37.98 & Adapting the neural encoder-decoder framework from single to multi-document summarization \Tstrut\Bstrut\\\hline
    Proposed System	& \textbf{28.17} & \textbf{26.94} & \Tstrut\Bstrut\\\hline
    \end{tabular}
\end{table}

The comparison of this system single document summarisation performance to state of the art in Table \ref{tab:singleCompare} shows that the proposed system lags behind state of the art in all three ROUGE metrics. The system does however seem to have the similar performance ratios between the three metrics, meaning that the systems implementation is only limited from the method used for summarisation and not limited from an incorrect implementation. It should be noted that the systems that achieve state of the art performance on the CNN/DailyMail dataset are extractive methods based on neural networks or reinforcement learning models. These methods also used the dataset for training and hence are completely tailored for summarisation of the articles contained in the CNN/DailyMail dataset. The proposed system used more traditional methods that have shown strong performance in producing summaries under the challenging conditions of multiple document and domain specific redundancy. Systems using classical approaches to summarisation have not been tested on the CNN/DailyMail dataset due to the dataset’s recent introduction. However a recent domain independent extractive method NMF-TR \citep{khurana2019extractive}, that isn’t based on supervised learning, has shown similar performance to the proposed unsupervised domain independent system. The relative performance of the proposed system to state of the art extractive techniques demonstrates the system ability to produce adequate single document summaries, and demonstrates correct implementation of existing methods.

The comparison of the proposed system of multi-document summarisation performance is given in Table \ref{tab:multiCompare}. The baseline systems included in the comparison are extractive multi document summarisation systems evaluated by the paper that introduced the MultiNews dataset \citep{fabbri2019multi}. Due to the recent release of this dataset there is a limited number of systems that have been evaluated on it.  While the system does not achieve state of the art performance, it ROUGE metrics are competitive with baseline systems. This not only demonstrates the efficacy of the system in performing multi document summaries, but also potential in performing state of the art domain independent summarisation with further improvements to components of the proposed system.

\subsection{Summary of Comparative Results}
The proposed system has been proven effective in summarisation and in its implementation of existing methods, from evaluation on single document and multi-document summarisation data sets. While the system does not achieve state of the art extractive summarisation performance, it is competitive with other extractive methods that use similar classical approaches to summarisation. Performance of the system is better for multi-document summarisation then single document summarisation, reflecting the effort made for the system to address the high redundancy of multiple documents in the same domain. The results of the comparative quantitative analysis assures the viability of the system to be used in summarisation of a personalised document set.

\section{Analysis 2: Efficacy of personalised Summarisation}
The aim of this evaluation is to assess the system ability to produce personalised summaries via queries. This system was designed to allow for the summarisation system to be used in a recommender system, specified by \textbf{R6}. To assess the system's ability to perform this task the system was tested on two different query types: a generic query and a contextual query based on a sample user knowledge model. Context free queries are simply an information request expressed as a query, contextual queries are information requests based on the context of the read document. Context free queries test the system's ability to retrieve documents based and produce a summary of those retrieved documents. Contextual queries assess the system ability to develop cross chain queries in exampasion and emulates the summarisation systems operation in a recommender system.

The two types of queries output were created by hand and used to produce summaries. The personalised summaries produced are examined but not evaluated quantitatively. Quantitative analysis requires a labeled dataset, which was unavailable and creation of one was beyond the scope of the project. Examination of summaries produced by the system demonstrates the systems ability to produce personalised summaries within a specific domain. The examination of the proposed system’s personalised summaries also highlights components of the system that could be improved in future work. The method for accessing the system ability to produce personalised summaries, the produced summaries, and discussion of system output is provided in this section.

\subsection{Method}
Context free and contextual queries were used for producing summaries for examination. These two types of summaries require two different approaches to evaluation. Context free queries simply require the construction of a query using concepts that are in the document set. Contextual queries aim to emulate recommender systems that the system was designed to accommodate.  Contextual queries require the creation of a query from a document being “read” by a user. The query created from a document is combined with a query from either a user or recommender system to provide a contextualised query. These two methods are described in this sub section.

\subsubsection{Context Free Query}
A context free query is a query without the addition of context from a document or a user model. Context free queries are constructed using concepts from the Corpus.concepts list in Corpus class in Corpus.py. Queries can be of any length, and their specificity is based on the concepts specificity to topic, described by the concepts probability relating to a topic in the topic model. The context free query “[ “nixon”, “impeachment” ]” was created by hand, and produced the result shown in Table \ref{tab:free}.

\begin{table}[h]
    \caption{System Output of Context Free Query}
    \label{tab:free}
    \centering
    \begin{tabular}{ Y } \hline \\
    \textbf{Original Query}: ['impeachment', 'nixon']\Tstrut\Bstrut\\ \\ \hline \\
    \textbf{Expanded Query}: ['trump', 'impeachment', 'nixon', 'john dean', 'dean', 'pardon', 'haldeman', 'woodward', 'august', 'washington', 'felt', 'oval office', 'congress']\Tstrut\Bstrut\\ \\ \hline \\ 
    \textbf{Summary}: The process was formally initiated on February 6, 1974, when the House granted the Judiciary Committee authority to investigate whether sufficient grounds existed to impeach President Nixon of high crimes and misdemeanors  under, of the United States Constitution.  The newly renamed facility, the Richard Nixon Presidential Library and Museum, now houses the tapes and releases additional tapes to the public periodically, which are available online and in the public domain.  This investigation was undertaken one year after the United States Senate established the Select Committee on Presidential Campaign Activities to investigate the 1972 break-in at the Democratic National Committee headquarters at the Watergate office complex in Washington, D. C. , and the Republican Nixon administration's attempted cover-up of its involvement; during those hearings the scope of the scandal became apparent and the existence of the Nixon White House tapes was revealed.  Acknowledging the reality of the situation, Charles Sandman lamented, "There is no way the outcome of this vote is going to be changed by debate. "\Tstrut\Bstrut\\ \\ \hline \\
    \textbf{Article Links and Titles}: \begin{itemize}
        \itemsep0em 
        \item Impeachment process against Richard Nixon: \url{https://en.wikipedia.org/wiki?curid=47279701}
        \item Watergate scandal:  \url{https://en.wikipedia.org/wiki?curid=52382}
        \item Nixon White House tapes:  \url{https://en.wikipedia.org/wiki?curid=3188742}
    \end{itemize}\Tstrut\Bstrut\\\hline
    \end{tabular}
\end{table}

\subsubsection{Contextual Query}
Contextual queries express an information need based on a given context. The context can either be given from a document that is being read (information related to a query in the context of current presented information) or from a knowledge recommendation given from a recommender system (information related to a query in the context of recommendation of information). The context chosen for creation of contextual queries was a paragraph from a document in the corpus set. The document and the concepts extracted are the following:

\textbf{Document being read}: "In the context of the Watergate scandal, Operation Gemstone was a proposed series of clandestine or illegal acts, first outlined by G. Gordon Liddy in two separate meetings with three other individuals: then-Attorney General of the United States, John N. Mitchell, then-White House Counsel John Dean, and Jeb Magruder, an ally and former aide to H.R. Haldeman, as well as the temporary head of the Committee to Re-elect the President, pending Mitchell's resignation as Attorney General."

\textbf{Concepts Extracted}: ['context', 'watergate scandal', 'operation gemstone', 'proposed series', 'clandestine', 'illegal acts', 'g. gordon liddy', 'separate meetings', 'other individuals', 'then-attorney general', 'united states', 'john n. mitchell', 'then-white house counsel john dean', 'jeb magruder', 'ally', 'former aide', 'h.r', 'haldeman', 'temporary head', 'committee', 'president', 'mitchell', 'resignation', 'attorney general', 'watergate', 'operation gemstone', 'united states', 'john n. mitchell', 'house counsel john dean', 'jeb magruder', 'committee', 'mitchell']

The document concepts were turned into a query by selecting the top n concepts that cover the content of the document. This was performed by assessing the similarity of extracted concepts topic distributions and the topic distribution of the document original document using cosine similarity. Thus the query produced from the context of the selected document is the following:

\textbf{Document context query}: ['president', 'committee', 'john n. mitchell', 'h.r', 'united states']

This context was combined with three different hand constructed context free queries, created as described in the context free method above. The context free queries and document context queries expanded using the Query class in Query.py. This process created three contextual queries. The three contextual queries created test the system's ability to retrieve relevant documents with nuanced personalised queries. It should be noted that this method is limited by the hand creation of context free queries. This method of creating queries fails to recognise the relationships of concepts to specific topics, queries that utilise concept topic relationships would have better results, and would be ideally created from a user knowledge model based on the topic model used in the summarisation system. The three contextual queries created are presented in Table \ref{tab:queries}. The result of these three queries are presented in Tables \ref{tab:u0}, \ref{tab:u1}, \& \ref{tab:u2}

\begin{table}[h]
    \caption{Contextual Queries}
    \label{tab:queries}
    \centering
    \begin{tabular}{| l | W | W | Z |} \hline
    \textbf{User} & \textbf{Context Free Query} & \textbf{Document Query} & \textbf{Contextual Query} \Tstrut\Bstrut\\\hline
    User0 & [ 'senate watergate committee', 'impeachment', 'testimony'] & ['president', 'committee', 'john n. mitchell', 'h.r', 'united states'] & ['president', 'committee', 'john n. mitchell', 'h.r', 'united states', 'senate watergate committee', 'impeachment', 'testimony', 'tapes', 'stone', 'oval office', 'senate', 'congress', 'white house', 'ford', 'house', 'richard nixon', 'nixon']\Tstrut\Bstrut\\\hline
    User1 & ['operation sandwedge', 'political enemies', 'caulfield'] & ['president', 'committee', 'john n. mitchell', 'h.r', 'united states'] & ['president', 'committee', 'john n. mitchell', 'h.r', 'united states', 'operation sandwedge', 'political enemies', 'caulfield', 'haig', 'dean', 'impeachment', 'watergate', 'stone', 'house', 'white house', 'nixon']\Tstrut\Bstrut\\\hline
    User2 & ['october', 'saturday night massacre', 'tapes'] & ['president', 'committee', 'john n. mitchell', 'h.r', 'united states'] & ['president', 'committee', 'john n. mitchell', 'h.r', 'united states', 'october', 'saturday night massacre', 'tapes', 'crp', 'house', 'richard nixon', 'impeachment', 'cox', 'white house', 'nixon', 'lee']\Tstrut\Bstrut\\\hline
    \end{tabular}
\end{table}

\RaggedRight
\begin{table}[h]
    \caption{System Output of User 0 Contextual Query}
    \label{tab:u0}
    \centering
    \begin{tabular}{ Y } \hline \\
    Original Query: ['senate watergate committee', 'impeachment','testimony'] \Tstrut\Bstrut\\ \\ \hline \\
    Content Query: ['president', 'committee', 'john n. mitchell', 'h.r', 'united states']\Tstrut\Bstrut\\ \\ \hline \\
    Contextual Query: ['president', 'committee', 'john n. mitchell', 'h.r', 'united states', 'senate watergate committee', 'impeachment', 'testimony', 'tapes', 'stone', 'oval office', 'senate', 'congress', 'white house', 'ford', 'house', 'richard nixon', 'nixon'] \Tstrut\Bstrut\\ \\ \hline \\ 
    Summary: ['On August 9, the Senate committee filed suit in federal district court to force President Nixon to make the subpoenaed tapes available.', 'In a "BoJack Horseman" second season episode called "The Shot", the title character and Todd visit the Nixon Presidential Library with the intent of stealing a scaled-down replica of the library.', 'A week later, Jaworski obtained a subpoena from Judge Sirica ordering Nixon to release 64 additional recordings in connection with his case.', 'The company purchased the that belonged to the defunct Chesapeake and Ohio Canal in February 1960 for 10 million.', 'The release of the "smoking gun" tape destroyed Nixon politically.', 'He was featured in Joseph Rodota\'s book "The Watergate: Inside America\'s Most Infamous Address."', 'Criminal prosecution was still a possibility at both the federal and the state level.', 'The apartment buildings included two-story units on the first and second floors, while the top-floor units had private rooftop terraces and fireplaces.', 'The Nixon White House tapes are audio recordings of conversations between U.S. President Richard Nixon and Nixon administration officials, Nixon family members, and White House staff, produced between 1971 and 1973.', 'The complex was the first mixed-use development in the District of Columbia, and was intended to help define the area as a business and residential rather than industrial district.']\Tstrut\Bstrut\\ \\ \hline \\ 
    Article Links and Title: \begin{itemize}
        \itemsep0em 
        \item Impeachment process against Richard Nixon:\url{https://en.wikipedia.org/wiki?curid=4727970}
        \item Nixon's Enemies List:\url{https://en.wikipedia.org/wiki?curid=390336}
        \item Watergate complex:\url{https://en.wikipedia.org/wiki?curid=625197}
        \item Watergate scandal:\url{https://en.wikipedia.org/wiki?curid=52382}
        \item Bruce Givner:\url{https://en.wikipedia.org/wiki?curid=55985550}
        \item Nixon White House tapes:\url{https://en.wikipedia.org/wiki?curid=3188742}
    \end{itemize}\Tstrut\Bstrut\\\hline
    \end{tabular}
\end{table}

\begin{table}[h]
    \caption{System Output of User 1 Contextual Query}
    \label{tab:u1}
    \centering
    \begin{tabular}{ Y } \hline \\
    Original Query: ['operation sandwedge', 'political enemies', 'caulfield'] \Tstrut\Bstrut\\ \\ \hline \\
    Content Query: ['president', 'committee', 'john n. mitchell', 'h.r', 'united states']\Tstrut\Bstrut\\ \\ \hline \\
    Contextual Query: ['president', 'committee', 'john n. mitchell', 'h.r', 'united states', 'operation sandwedge', 'political enemies', 'caulfield', 'haig', 'dean', 'impeachment', 'watergate', 'stone', 'house', 'white house', 'nixon'] \Tstrut\Bstrut\\ \\ \hline \\ 
    Summary: ['This group greatly increased the strength of Northerners and liberals in the House Democratic Caucus.', 'He dubbed the secret informant "Deep Throat", alluding to both the deep background status of his information and the widely publicized 1972 pornographic film "Deep Throat".', 'Of the transcripts released, Nixon said: "They include all the relevant portions of all of the subpoenaed conversations that were recorded—that is, all portions that relate to the question of what I knew about Watergate or the cover-up and what I did about it."', 'Article II, charging Nixon with abuse of power, alleged in part that:', "The Watergate Scandal refers to the burglary and illegal wiretapping of the Washington, D.C. headquarters of the Democratic National Committee, in the Watergate complex, by members of President of the United States Richard Nixon's re-election committee and subsequent abuse of powers by the president and administration officials to halt or hinder the investigation into same.", 'After Nixon won the 1972 presidential election, Stone worked for the administration in the Office of Economic Opportunity.', 'You backstab your friends-run your mouth my lawyers are dying Rip you to shreds."']\Tstrut\Bstrut\\ \\ \hline \\ 
    Article Links and Title: \begin{itemize}
        \itemsep0em 
        \item Watergate Babies:\url{https://en.wikipedia.org/wiki?curid=5413448}
        \item Deep Throat (Watergate):\url{https://en.wikipedia.org/wiki?curid=461561} 
        \item Impeachment process against Richard Nixon:\url{https://en.wikipedia.org/wiki?curid=47279701} 
        \item Timeline of the Watergate scandal:\url{https://en.wikipedia.org/wiki?curid=2090607} 
        \item Roger Stone:\url{https://en.wikipedia.org/wiki?curid=1723963} 
    \end{itemize}\Tstrut\Bstrut\\\hline
    \end{tabular}
\end{table}

\begin{table}[h]
    \caption{System Output of User 2 Contextual Query}
    \label{tab:u2}
    \centering
    \begin{tabular}{ Y } \hline \\
    Original Query: ['october', 'saturday night massacre', 'tapes'] \Tstrut\Bstrut\\ \\ \hline \\
    Content Query: ['president', 'committee', 'john n. mitchell', 'h.r', 'united states']\Tstrut\Bstrut\\ \\ \hline \\
    Contextual Query: ['president', 'committee', 'john n. mitchell', 'h.r', 'united states', 'october', 'saturday night massacre', 'tapes', 'crp', 'house', 'richard nixon', 'impeachment', 'cox', 'white house', 'nixon', 'lee'] \Tstrut\Bstrut\\ \\ \hline \\ 
    Summary: ['Ford had become Vice President on December 6, 1973, after the resignation of Spiro Agnew.', 'Hunt and Liddy recommended a "covert operation" to get a "mother lode" of information about Ellsberg\'s mental state in order to discredit him.', 'Daniel Ellsberg', 'Gertrude, a peripatetic nun, only accessible on the telephone at her own convenience.', 'If they follow orders, they may become complicit in starting an unnecessary war.', 'His presence substantially delayed the break-in and indirectly led to the eventual arrests of the burglars.', 'It contained 260 residential units, more than any other building in the complex.', 'Exposing official lies could however carry a heavy personal cost as they could be imprisoned for unlawful disclosure of classified information.', 'In the late 1980s and early 1990s, Richardson was associated with the Washington, D.C., office of the New York City law firm of Milbank, Tweed, Hadley \& McCloy, of which John J. McCloy was a founding partner.', 'The investigation into the burglary revealed that high officials in the administration of President Richard Nixon had ordered the break-in and then tried to cover up their involvement.', 'During a second burglary on June 17, 1972, to replace a malfunctioning phone tap and collect more information, five of the burglars were arrested and the Watergate scandal began to unfold.']\Tstrut\Bstrut\\ \\ \hline \\ 
    Article Links and Title: \begin{itemize}
        \itemsep0em 
        \item Inauguration of Gerald Ford:\url{https://en.wikipedia.org/wiki?curid=21193997} 
        \item Daniel Ellsberg:\url{https://en.wikipedia.org/wiki?curid=80128}
        \item The Abbess of Crewe:\url{https://en.wikipedia.org/wiki?curid=52342305} 
        \item Bruce Givner:\url{https://en.wikipedia.org/wiki?curid=55985550} 
        \item Watergate complex:\url{https://en.wikipedia.org/wiki?curid=625197} 
        \item Elliot Richardson:\url{https://en.wikipedia.org/wiki?curid=324039} 
    \end{itemize}\Tstrut\Bstrut\\\hline
    \end{tabular}
\end{table}
\justify

\subsection{Results \& Observations}
What is immediately clear in the comparison of context free and contextual query generated summaries, is the difference in interoperability. The summary in Table \ref{tab:free}, produced from the context free query, is much more readable and seems to cover more relevant material, while the summaries in Table \ref{tab:u0}, \ref{tab:u1}, and \ref{tab:u2}, produced from contextual queries, are less readable and are less relevant. From the comparative evaluation, results show that the system is able to perform competitively to other extractive systems. Therefore the issues with relevancy of summary content and readability are a result both the corpus used in this evaluation and the document retrieval component of the system.

Issues with both the corpus and topic model representation for retrieval were presented in the discussion of existing retrieval methods in Section \ref{subsec:3.5.3}. These problems inherent to the methods used in the system, can explain the difference in context free and contextual query-based summaries produced by the system. One of limitations that was discussed in the design chapter was decreased topic model accuracy when a small corpus is used \citep{crossley2017important}. The corpus used for summary testing was a small set of 32 Wikipedia articles relating to the Watergate Scandal. The decreased accuracy of the topic model, from the small corpus set used, means that the topic distribution of documents and queries are also less accurate. The topic distributions of documents and queries are how the system was able to determine documents relevance to a query. With the decreased accuracy in representation, the set of documents retrieved for a query are less accurate, resulting in the summary being less accurate. The context free query isn’t as taxing on the system intermediate topic model representation of the corpus. This is because context free query uses a smaller more centralised set of concepts as the query. A smaller more centralised query is more likely to have a more centralised topic distribution and therefore it is easier to find documents of similarity as they will be centralised around that topic. The contextual queries contrastingly have both larger and less centralised concepts sets as queries. Therefore the topic distribution of these queries is likely more dispersed, increasing the reliance on topic model accuracy and limiting the number of documents that are found relevant. This is also due to the context free part of the contextual query being generated from hand and disregarding context to topic relationships. Another issue presented in selection of a document retrieval method (Section \ref{subsec:3.5.3}) was that direct implementations of topic modeling can be too coarse for document retrieval \citep{wei2006lda}. Using a concept based LDA model was an attempt to make the topic model more fine grained but these methods may still be too direct of an implementation of LDA and therefore too coarse for retrieval of documents. A less direct implementation of LDA may solve issues with large dispersed concept sets present in contextual queries.

Another limitation from the corpus can be observed in the inclusion of non-informative material in summaries, seen both in the sentences used as well as the relevant article titles and links included with the summary. For example in Table \ref{tab:u0} the summary includes a sentence discussing a television show which portrayed the events of the Watergate Scandal. Another example can be seen in the inclusion of the Wikipedia article “The Abbess of Crewe”, a novel based on the allegorical treatment of the watergate scandal, shown in Table \ref{tab:u2}. While both examples are related to the query and to the set domain of corpus documents, they are not informative and thus their inclusion reduces the efficacy of the personalised summary. Wikipedia’s encyclopedic style of content means that related articles to a domain may not solely contain content related to a domain that the article relates to. The inclusion of non-informative material is a result of the lack of preprocessing of material contained in the corpus. This system is unsupervised and extractive. These classifications means the systems topic representation as well as content to be contained in the summary is solely based on the corpus documents. Thus there is a need for the system to be able to preprocess a corpus to ensure content is informative and representative of the domain, if to be used for informative summarisation.

Despite some of the issues observed in the summaries produced from contextual queries, the summary produced from the context free query is readable and relevant. Clearly demonstrating the systems ability to produce a personalised summary. The results of contextual queries also demonstrate the system ability to produce different summaries from a shared context. The limitations in producing these personalised summaries are not a result of the overall system, but rather just the retrieval component of the system, thus improvement to document retrieval could be made to increase the efficacy of personalised summaries. This systems generation of summaries personalised to queries offers that this system could effectively produce summaries based on personalised queries if housed in a recommender system. 

\subsection{Summary of Personalised Summary Evaluation}
The evaluation of context free and contextual queries for producing summaries has highlighted limitations of  the methods used in document retrieval, such as topic representation of small corpus sets and inclusion of raw encyclopedic information. The evaluation has also demonstrated the system effectiveness in providing personalised summaries when operating within its limitations, such as the effectiveness of context free query based summaries, and difference of summaries from a shared context. Due to an unlabeled dataset, quantitative or comparative analysis was unable to be performed. Without this it is not known how well the system performs relative to other personalised text summarisation systems. Despite this, the summaries generated by the system are tailored to the information depicted in the given query, demonstrating the system's ability to produce personalised summaries.

\section{Evaluation Summary}
The comparison of summarisation performance with state of the art extractive systems as well as examination of system summaries with using queries for personalise summarisation has demonstrated the existing extractive methods can be combined to effectively implement a method of performing domain specific personalised summarisation independent of domain specific models. The result of the comparison of ROUGE metrics of the proposed system on both single document and multi-document datasets against state of the art extractive summarisation systems is the proposed system is a competitive method for summarisation. This also assures that the problems observed in personalised summaries are limitations of the methods used on corpus that the system was tested on. From results of this evaluation the systems design and implementation can be reviewed in achieving the requirements outlayed from Chapter \ref{chp:3}. The requirements and the conclusions determined from this evaluation are the following.

\textbf{R1}: The system operates on specific domain material without the use of formal supervised domain models.

The system was able to produce a readable and relevant summary when given a context free query in the domain of the Watergate Scandal, in Analysis 2. Thus this system is able to operate on specific domain material without the need of formal supervised domain models

\textbf{R2}: The system forms summaries which significantly reduce the original content, while maintaining the most salient content, reducing the effects of information overload.

The system demonstrated competitive performance in its evaluation on the MultiNews multi-document summarisation dataset in Analysis 1. This data set required summaries to be produced across a large amount of documents. Thus the system is able to significantly reduce original content while maintaining the most salient content, satisfying this requirement.

\textbf{R3}: The system produces interpretable output that enhances user processing capabilities of the information being summarised.

The output of Analysis 2 examined summaries and the relevant links to which the summary was formed. The system also outputs the expanded query of the system. These two outputs give explanibility to the system formation of the summary. The expand query can also be used by a user to adjust the terms they included to express their true information need. Therefore it can be concluded that the system produces output which is interprobable to enhance a users processing capabilities.

\textbf{R4}: The system provides summaries which are personalised to a user's information needs.

The evaluation of context-free and contextual queries in Analysis 2, demonstrates this system's ability to understand term relations seen in the expansion of queries and the resulting summary. Thus this system provides term and topic modeling to perform summarisation on specific domain material and address both the content of the original query and the latent topics that connect them.

\textbf{R5}: The system is constructed from existing extractive methods of summarisation that use topic representations as their immediate representation of source documents.

The system that was evaluated here used a LDA topic model representation.

\textbf{R6}: The system is designed to be used with topic model based recommender systems.

The examination of contextual queries in Analysis 2, demonstrates this system ability to be used with recommender systems, as the context that was used in contextual queries could be provided by a recommender system. A recommender system could also generate a query that could be combined with the context of a document being read. Therefore the system could be used within a topic model based recommender system.

Thus it can be concluded that the proposed system satisfies the requirements outlined from the motivation and research question of this project proving it is an effective system to perform personalised domain specific summaries, without the need of domain models.
